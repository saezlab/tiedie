{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"TieDIE: Tied Diffusion for Subnetwork Discovery","text":"<p>A network analysis algorithm that finds subnetworks connecting genomic perturbations to transcriptional changes in large gene interaction networks.</p>"},{"location":"#authors","title":"Authors","text":"<p>Evan O. Paull, Daniel Carlin and Joshua M. Stuart (UC Santa Cruz)</p>"},{"location":"#additional-contributors","title":"Additional Contributors","text":"<ul> <li>Srikanth Bezawada (TieDIE Cytoscape Plugin)</li> <li>Josh L. Espinoza (Quick kernel loading feature)</li> <li>Dana Silverbush (MATLAB kernel generation code updates)</li> <li>Denes Turei (modern tooling and packaging)</li> </ul>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python &gt;= 3.9</li> <li>numpy &gt;= 1.20</li> <li>scipy &gt;= 1.0</li> <li>networkx &gt;= 2.0</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-pip-from-github","title":"Using pip (from GitHub)","text":"<pre><code>pip install git+https://github.com/saezlab/tiedie.git\n</code></pre>"},{"location":"#using-uv-recommended","title":"Using uv (recommended)","text":"<p>uv is a fast Python package manager. To install TieDIE with uv:</p> <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create a new project with TieDIE\nuv init my-project\ncd my-project\nuv add git+https://github.com/saezlab/tiedie.git\n\n# Or add to an existing project\nuv add git+https://github.com/saezlab/tiedie.git\n</code></pre>"},{"location":"#development-installation","title":"Development installation","text":"<pre><code>git clone https://github.com/saezlab/tiedie.git\ncd tiedie\n\n# Using uv (recommended)\nuv venv\nsource .venv/bin/activate\nuv pip install -e \".[tests]\"\n\n# Or using pip\npython -m venv .venv\nsource .venv/bin/activate\npip install -e \".[tests]\"\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#command-line-interface","title":"Command Line Interface","text":"<pre><code>tiedie -n pathway.sif -u upstream.input -d downstream.input\n</code></pre> <p>For full options:</p> <pre><code>tiedie --help\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>from tiedie import ScipyKernel\nfrom tiedie.util import parse_heats, parse_net, normalize_heats\n\n# Parse input files\nnetwork = parse_net('pathway.sif')\nupstream_heats, upstream_signs = parse_heats('upstream.input')\ndownstream_heats, downstream_signs = parse_heats('downstream.input')\n\n# Normalize heats\nupstream_norm = normalize_heats(upstream_heats)\ndownstream_norm = normalize_heats(downstream_heats)\n\n# Create diffusion kernel and run diffusion\ndiffuser = ScipyKernel('pathway.sif')\nup_diffused = diffuser.diffuse(upstream_norm, reverse=False)\ndown_diffused = diffuser.diffuse(downstream_norm, reverse=True)\n</code></pre>"},{"location":"#input-file-formats","title":"Input File Formats","text":""},{"location":"#network-file-sif","title":"Network file (.sif)","text":"<p>Tab-separated: <code>source \\t interaction \\t target</code></p> <pre><code>GeneA   -a&gt; GeneB\nGeneB   -t| GeneC\n</code></pre> <p>Interaction types: - <code>-a&gt;</code> : activation - <code>-t|</code> : inhibition - <code>-component&gt;</code> : component relationship</p>"},{"location":"#heat-file-input","title":"Heat file (.input)","text":"<p>Tab-separated: <code>gene \\t heat \\t sign</code></p> <pre><code>GeneA   10.5    +\nGeneB   8.2 -\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>tiedie/\n\u251c\u2500\u2500 __init__.py      # Public API\n\u251c\u2500\u2500 cli.py           # Command-line interface\n\u251c\u2500\u2500 kernel.py        # Pre-computed kernel diffusion\n\u251c\u2500\u2500 kernel_scipy.py  # On-the-fly kernel generation (scipy)\n\u251c\u2500\u2500 ppr.py           # Personalized PageRank diffusion\n\u251c\u2500\u2500 permute.py       # Permutation testing\n\u251c\u2500\u2500 util.py          # Utility functions\n\u2514\u2500\u2500 ...\ntests/               # Test suite\ndocs/                # Documentation (Tutorial.pdf, FAQ.txt)\n</code></pre>"},{"location":"#publications","title":"Publications","text":"<p>TieDIE was first featured in the 2013 Nature paper \"Comprehensive molecular characterization of clear cell renal cell carcinoma\" (TCGA Network). In this publication, TieDIE analysis connected frequently mutated genes involving the SWI/SNF chromatin remodelling complex to gene expression changes characteristic of tumor development and progression. The TieDIE network solution is shown in Figure 4: http://www.nature.com/nature/journal/v499/n7456/full/nature12222.html</p>"},{"location":"#license","title":"License","text":"<p>GPL-3.0-or-later</p>"},{"location":"#contact","title":"Contact","text":"<p>Feature requests, comments and requests for clarification should be sent to epaull@soe.ucsc.edu.</p>"},{"location":"#development","title":"Development","text":""},{"location":"#running-tests","title":"Running tests","text":"<pre><code># Install test dependencies\nuv pip install -e \".[tests]\"\n\n# Run tests\npytest\n\n# Run tests with coverage\npytest --cov=tiedie\n</code></pre>"},{"location":"#linting-and-formatting","title":"Linting and formatting","text":"<p>The project uses ruff for linting and formatting:</p> <pre><code># Check for issues\nruff check .\n\n# Auto-fix issues\nruff check --fix .\n\n# Format code\nruff format .\n</code></pre>"},{"location":"#pre-commit-hooks","title":"Pre-commit hooks","text":"<p>Pre-commit hooks are configured for automated checks:</p> <pre><code># Install pre-commit hooks\nuv pip install -e \".[dev]\"\npre-commit install\n\n# Run hooks manually on all files\npre-commit run --all-files\n</code></pre>"},{"location":"#building-documentation","title":"Building documentation","text":"<p>Documentation is built with MkDocs:</p> <pre><code># Install docs dependencies\nuv pip install -e \".[docs]\"\n\n# Serve docs locally\nmkdocs serve\n\n# Build static docs\nmkdocs build\n</code></pre>"},{"location":"#version-20-notes","title":"Version 2.0 Notes","text":"<p>This version represents a major modernization of the TieDIE codebase:</p> <ul> <li>Python 3 compatibility: Ported from Python 2.7 to Python 3.9+</li> <li>Modern packaging: Converted to a proper Python package with <code>pyproject.toml</code>   and hatchling build backend</li> <li>Test suite: Added pytest-based test suite with integration tests</li> <li>uv support: Compatible with modern Python tooling including uv</li> <li>CLI improvements: Migrated from optparse to argparse</li> </ul> <p>This modernization was done at Saez Lab by D\u00e9nes T\u00fcrei (turei.denes@gmail.com).</p>"},{"location":"reference/api/","title":"API Reference","text":""},{"location":"reference/api/#main-classes","title":"Main Classes","text":""},{"location":"reference/api/#scipykernel","title":"ScipyKernel","text":"<p>On-the-fly heat diffusion kernel using scipy matrix exponentiation.</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>class ScipyKernel:\n    \"\"\"On-the-fly heat diffusion kernel using scipy matrix exponentiation.\"\"\"\n\n    def __init__(self, network_file):\n        \"\"\"Input:\n\n            network_file - a tab-delimited file in .sif network format:\n            &lt;source&gt; &lt;interaction&gt; &lt;target&gt;\n\n        Returns:\n            Kernel object.\n\n        \"\"\"\n\n        self.labels = {}\n        # The number of rows and columns for each kernel\n        self.ncols = {}\n        self.nrows = {}\n\n        # parse the network, build indexes\n        edges, nodes, node_out_degrees = self.parse_net(network_file)\n        num_nodes = len(nodes)\n        node_order = list(nodes)\n        index2node = {}\n        node2index = {}\n        for i in range(0, num_nodes):\n            index2node[i] = node_order[i]\n            node2index[node_order[i]] = i\n\n        # construct the diagonals\n        # SCIPY uses row and column indexes to build the matrix\n        # row and columns are just indexes: the data column stores\n        # the actual entries of the matrix\n        row = array('i')\n        col = array('i')\n        data = array('f')\n        # build the diagonals, including the out-degree\n        for i in range(0, num_nodes):\n            # diag entries: out degree\n            degree = 0\n            if index2node[i] in node_out_degrees:\n                degree = node_out_degrees[index2node[i]]\n            # append to the end\n            # array object: first argument is the index, the second is the data value\n            # append the out-degree to the data array\n            data.insert(len(data), degree)\n            # build the diagonals\n            row.insert(len(row), i)\n            col.insert(len(col), i)\n\n        # add off-diagonal edges\n        for i in range(0, num_nodes):\n            for j in range(0, num_nodes):\n                if i == j:\n                    continue\n                if (index2node[i], index2node[j]) not in edges:\n                    continue\n                # append index to i-th row, j-th column\n                row.insert(len(row), i)\n                col.insert(len(col), j)\n                # -1 for laplacian: i.e. the negative of the adjacency matrix\n                data.insert(len(data), -1)\n\n        # Build the graph laplacian: the CSC matrix provides a sparse matrix format\n        # that can be exponentiated efficiently\n        L = coo_matrix((data, (row, col)), shape=(num_nodes, num_nodes)).tocsc()\n        time_T = -0.1\n        self.laplacian = L\n        self.index2node = index2node\n        # this is the matrix exponentiation calculation.\n        # Uses the Pade approximiation for accurate approximation. Computationally expensive.\n        # O(n^2), n= # of features, in memory as well.\n        self.kernel = expm(time_T * L)\n        self.labels = node_order\n\n        # self.print_laplacian()\n\n    def get_labels(self):\n        \"\"\"Return the set of all node/gene labels used by this kernel object\"\"\"\n        # all_labels = set()\n        # for label in self.labels:\n        all_labels = set(self.labels)\n\n        return all_labels\n\n    def print_laplacian(self):\n        \"\"\"Debug function\"\"\"\n        cx = self.laplacian.tocoo()\n        for i, j, v in zip(cx.row, cx.col, cx.data):\n            a = self.index2node[i]\n            b = self.index2node[j]\n            print('\\t'.join([a, b, str(v)]))\n\n    def parse_net(self, network):\n        \"\"\"Parse .sif network, using just the first and third columns\n        to build an undirected graph. Store the node out-degrees\n        in an index while we're at it.\n        \"\"\"\n        edges = set()\n        nodes = set()\n        degrees = {}\n        for line in open(network):\n            parts = line.rstrip().split('\\t')\n            source = parts[0]\n            target = parts[2]\n\n            # if inputing a multi-graph, skip this\n            if (source, target) in edges:\n                continue\n\n            edges.add((source, target))\n            edges.add((target, source))\n            nodes.add(source)\n            nodes.add(target)\n\n            if source not in degrees:\n                degrees[source] = 0\n            if target not in degrees:\n                degrees[target] = 0\n\n            degrees[source] += 1\n            degrees[target] += 1\n\n        return (edges, nodes, degrees)\n\n    def kernel_multiply_one(self, vector):\n        \"\"\"Multiply the specified kernel by the supplied input heat vector.\n\n        Input:\n            vector: A hash mapping gene labels to floating point values\n            kernel: a single index for a specific kernel\n\n        Returns:\n            A hash of diffused heats, indexed by the same names as the\n            input vector\n        \"\"\"\n        # Have to convert to ordered array format for the input vector\n        array = []\n        for label in self.labels:\n            # Input heats may not actually be in the network.\n            # Check and initialize to zero if not\n            if label in vector:\n                array.append(vector[label])\n            else:\n                array.append(0)\n\n        # take the dot product\n        value = self.kernel * array\n\n        # Convert back to a hash and return diffused heats\n        return_vec = {}\n        idx = 0\n        for label in self.labels:\n            return_vec[label] = float(value[idx])\n            idx += 1\n\n        return return_vec\n\n    def diffuse(self, vector, reverse=False):\n        \"\"\"Diffuse input heats over the set of kernels, add to this object\n\n        Input:\n            {'gene1': float(heat1)\n             'gene2' : float(heat2)\n              ...\n            }\n\n        Returns:\n            Diffused heat vector\n        \"\"\"\n\n        diffused_vector = self.kernel_multiply_one(vector)\n\n        return diffused_vector\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel-functions","title":"Functions","text":""},{"location":"reference/api/#tiedie.ScipyKernel.__init__","title":"<code>__init__(network_file)</code>","text":"<p>Input:</p> <pre><code>network_file - a tab-delimited file in .sif network format:\n&lt;source&gt; &lt;interaction&gt; &lt;target&gt;\n</code></pre> <p>Returns:</p> Type Description <p>Kernel object.</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def __init__(self, network_file):\n    \"\"\"Input:\n\n        network_file - a tab-delimited file in .sif network format:\n        &lt;source&gt; &lt;interaction&gt; &lt;target&gt;\n\n    Returns:\n        Kernel object.\n\n    \"\"\"\n\n    self.labels = {}\n    # The number of rows and columns for each kernel\n    self.ncols = {}\n    self.nrows = {}\n\n    # parse the network, build indexes\n    edges, nodes, node_out_degrees = self.parse_net(network_file)\n    num_nodes = len(nodes)\n    node_order = list(nodes)\n    index2node = {}\n    node2index = {}\n    for i in range(0, num_nodes):\n        index2node[i] = node_order[i]\n        node2index[node_order[i]] = i\n\n    # construct the diagonals\n    # SCIPY uses row and column indexes to build the matrix\n    # row and columns are just indexes: the data column stores\n    # the actual entries of the matrix\n    row = array('i')\n    col = array('i')\n    data = array('f')\n    # build the diagonals, including the out-degree\n    for i in range(0, num_nodes):\n        # diag entries: out degree\n        degree = 0\n        if index2node[i] in node_out_degrees:\n            degree = node_out_degrees[index2node[i]]\n        # append to the end\n        # array object: first argument is the index, the second is the data value\n        # append the out-degree to the data array\n        data.insert(len(data), degree)\n        # build the diagonals\n        row.insert(len(row), i)\n        col.insert(len(col), i)\n\n    # add off-diagonal edges\n    for i in range(0, num_nodes):\n        for j in range(0, num_nodes):\n            if i == j:\n                continue\n            if (index2node[i], index2node[j]) not in edges:\n                continue\n            # append index to i-th row, j-th column\n            row.insert(len(row), i)\n            col.insert(len(col), j)\n            # -1 for laplacian: i.e. the negative of the adjacency matrix\n            data.insert(len(data), -1)\n\n    # Build the graph laplacian: the CSC matrix provides a sparse matrix format\n    # that can be exponentiated efficiently\n    L = coo_matrix((data, (row, col)), shape=(num_nodes, num_nodes)).tocsc()\n    time_T = -0.1\n    self.laplacian = L\n    self.index2node = index2node\n    # this is the matrix exponentiation calculation.\n    # Uses the Pade approximiation for accurate approximation. Computationally expensive.\n    # O(n^2), n= # of features, in memory as well.\n    self.kernel = expm(time_T * L)\n    self.labels = node_order\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel.diffuse","title":"<code>diffuse(vector, reverse=False)</code>","text":"<p>Diffuse input heats over the set of kernels, add to this object</p> Input <p>{'gene1': float(heat1)  'gene2' : float(heat2)   ... }</p> <p>Returns:</p> Type Description <p>Diffused heat vector</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def diffuse(self, vector, reverse=False):\n    \"\"\"Diffuse input heats over the set of kernels, add to this object\n\n    Input:\n        {'gene1': float(heat1)\n         'gene2' : float(heat2)\n          ...\n        }\n\n    Returns:\n        Diffused heat vector\n    \"\"\"\n\n    diffused_vector = self.kernel_multiply_one(vector)\n\n    return diffused_vector\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel.get_labels","title":"<code>get_labels()</code>","text":"<p>Return the set of all node/gene labels used by this kernel object</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def get_labels(self):\n    \"\"\"Return the set of all node/gene labels used by this kernel object\"\"\"\n    # all_labels = set()\n    # for label in self.labels:\n    all_labels = set(self.labels)\n\n    return all_labels\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel.kernel_multiply_one","title":"<code>kernel_multiply_one(vector)</code>","text":"<p>Multiply the specified kernel by the supplied input heat vector.</p> Input <p>vector: A hash mapping gene labels to floating point values kernel: a single index for a specific kernel</p> <p>Returns:</p> Type Description <p>A hash of diffused heats, indexed by the same names as the</p> <p>input vector</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def kernel_multiply_one(self, vector):\n    \"\"\"Multiply the specified kernel by the supplied input heat vector.\n\n    Input:\n        vector: A hash mapping gene labels to floating point values\n        kernel: a single index for a specific kernel\n\n    Returns:\n        A hash of diffused heats, indexed by the same names as the\n        input vector\n    \"\"\"\n    # Have to convert to ordered array format for the input vector\n    array = []\n    for label in self.labels:\n        # Input heats may not actually be in the network.\n        # Check and initialize to zero if not\n        if label in vector:\n            array.append(vector[label])\n        else:\n            array.append(0)\n\n    # take the dot product\n    value = self.kernel * array\n\n    # Convert back to a hash and return diffused heats\n    return_vec = {}\n    idx = 0\n    for label in self.labels:\n        return_vec[label] = float(value[idx])\n        idx += 1\n\n    return return_vec\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel.parse_net","title":"<code>parse_net(network)</code>","text":"<p>Parse .sif network, using just the first and third columns to build an undirected graph. Store the node out-degrees in an index while we're at it.</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def parse_net(self, network):\n    \"\"\"Parse .sif network, using just the first and third columns\n    to build an undirected graph. Store the node out-degrees\n    in an index while we're at it.\n    \"\"\"\n    edges = set()\n    nodes = set()\n    degrees = {}\n    for line in open(network):\n        parts = line.rstrip().split('\\t')\n        source = parts[0]\n        target = parts[2]\n\n        # if inputing a multi-graph, skip this\n        if (source, target) in edges:\n            continue\n\n        edges.add((source, target))\n        edges.add((target, source))\n        nodes.add(source)\n        nodes.add(target)\n\n        if source not in degrees:\n            degrees[source] = 0\n        if target not in degrees:\n            degrees[target] = 0\n\n        degrees[source] += 1\n        degrees[target] += 1\n\n    return (edges, nodes, degrees)\n</code></pre>"},{"location":"reference/api/#tiedie.ScipyKernel.print_laplacian","title":"<code>print_laplacian()</code>","text":"<p>Debug function</p> Source code in <code>tiedie/kernel_scipy.py</code> <pre><code>def print_laplacian(self):\n    \"\"\"Debug function\"\"\"\n    cx = self.laplacian.tocoo()\n    for i, j, v in zip(cx.row, cx.col, cx.data):\n        a = self.index2node[i]\n        b = self.index2node[j]\n        print('\\t'.join([a, b, str(v)]))\n</code></pre>"},{"location":"reference/api/#kernel","title":"Kernel","text":"<p>Pre-computed heat diffusion kernel for network analysis.</p> Source code in <code>tiedie/kernel.py</code> <pre><code>class Kernel:\n    \"\"\"Pre-computed heat diffusion kernel for network analysis.\"\"\"\n\n    def __init__(self, kernel_files):\n        \"\"\"Input:\n\n            kernel_file - a tab-delimited matrix file with both a header\n            and first-row labels, in the same order.\n\n        Returns:\n                Kernel object.\n        \"\"\"\n\n        # Multiple kernels are supported. Linearity of the diffusion kernel allows\n        # this feature.\n\n        # store kernel object\n        self.kernels = {}\n        # Store the header line for each kernel: will be used for lookup\n        self.labels = {}\n        # The number of rows and columns for each kernel\n        self.ncols = {}\n        self.nrows = {}\n        # parse each kernel file\n        for kernel in kernel_files.split(':'):\n            # numpy's genfromtxt format. Relatively memory-intensive\n            # FIXME: add option for matlab .mat compressed file format\n            self.kernels[kernel] = genfromtxt(kernel, delimiter='\\t')[1:, 1:]\n            self.labels[kernel] = None\n            fh = open(kernel)\n            # get the header line\n            for line in fh:\n                self.labels[kernel] = line.rstrip().split('\\t')[1:]\n                break\n            fh.close()\n\n            self.ncols[kernel] = self.kernels[kernel].shape[1] - 1\n            self.nrows[kernel] = self.kernels[kernel].shape[0] - 1\n\n    def get_labels(self):\n        \"\"\"Return the set of all node/gene labels used by this kernel object\"\"\"\n        all_labels = set()\n        for label in self.labels:\n            all_labels = all_labels.union(set(self.labels[label]))\n\n        return all_labels\n\n    def kernel_multiply_one(self, kernel, vector):\n        \"\"\"Multiply the specified kernel by the supplied input heat vector.\n\n        Input:\n            vector: A hash mapping gene labels to floating point values\n            kernel: a single index for a specific kernel\n\n        Returns:\n            A hash of diffused heats, indexed by the same names as the\n            input vector\n        \"\"\"\n\n        # Have to convert to ordered array format for the input vector\n        array = []\n        for label in self.labels[kernel]:\n            # Input heats may not actually be in the network.\n            # Check and initialize to zero if not\n            if label in vector:\n                array.append(vector[label])\n            else:\n                array.append(0)\n\n        # Matrix mulitply op\n        value = dot(self.kernels[kernel], array)\n\n        # Convert back to a hash and return diffused heats\n        return_vec = {}\n        idx = 0\n        for label in self.labels[kernel]:\n            return_vec[label] = float(value[idx])\n            idx += 1\n\n        return return_vec\n\n    def add_vectors(self, vector_list):\n        \"\"\"Sum vectors: Add hash / float-valued vectors\"\"\"\n        sum = {}\n\n        for vec in vector_list:\n            for key in vec:\n                val = vec[key]\n                if key not in sum:\n                    sum[key] = val\n                else:\n                    sum[key] += val\n\n        return sum\n\n    def diffuse(self, vector, reverse=False):\n        \"\"\"Diffuse input heats over the set of kernels, add to this object\n\n        Input:\n            {'gene1': float(heat1)\n             'gene2' : float(heat2)\n              ...\n            }\n\n        Returns:\n            Diffused heat vector\n        \"\"\"\n        # diffuse separately on each kernel (if more than one), and store.\n        return_vectors = []\n        for kernel in self.kernels:\n            # run diffusion on each constituent kernel\n            diffused_vector = self.kernel_multiply_one(kernel, vector)\n            return_vectors.append(diffused_vector)\n\n        return self.add_vectors(return_vectors)\n</code></pre>"},{"location":"reference/api/#tiedie.Kernel-functions","title":"Functions","text":""},{"location":"reference/api/#tiedie.Kernel.__init__","title":"<code>__init__(kernel_files)</code>","text":"<p>Input:</p> <pre><code>kernel_file - a tab-delimited matrix file with both a header\nand first-row labels, in the same order.\n</code></pre> <p>Returns:</p> Type Description <p>Kernel object.</p> Source code in <code>tiedie/kernel.py</code> <pre><code>def __init__(self, kernel_files):\n    \"\"\"Input:\n\n        kernel_file - a tab-delimited matrix file with both a header\n        and first-row labels, in the same order.\n\n    Returns:\n            Kernel object.\n    \"\"\"\n\n    # Multiple kernels are supported. Linearity of the diffusion kernel allows\n    # this feature.\n\n    # store kernel object\n    self.kernels = {}\n    # Store the header line for each kernel: will be used for lookup\n    self.labels = {}\n    # The number of rows and columns for each kernel\n    self.ncols = {}\n    self.nrows = {}\n    # parse each kernel file\n    for kernel in kernel_files.split(':'):\n        # numpy's genfromtxt format. Relatively memory-intensive\n        # FIXME: add option for matlab .mat compressed file format\n        self.kernels[kernel] = genfromtxt(kernel, delimiter='\\t')[1:, 1:]\n        self.labels[kernel] = None\n        fh = open(kernel)\n        # get the header line\n        for line in fh:\n            self.labels[kernel] = line.rstrip().split('\\t')[1:]\n            break\n        fh.close()\n\n        self.ncols[kernel] = self.kernels[kernel].shape[1] - 1\n        self.nrows[kernel] = self.kernels[kernel].shape[0] - 1\n</code></pre>"},{"location":"reference/api/#tiedie.Kernel.add_vectors","title":"<code>add_vectors(vector_list)</code>","text":"<p>Sum vectors: Add hash / float-valued vectors</p> Source code in <code>tiedie/kernel.py</code> <pre><code>def add_vectors(self, vector_list):\n    \"\"\"Sum vectors: Add hash / float-valued vectors\"\"\"\n    sum = {}\n\n    for vec in vector_list:\n        for key in vec:\n            val = vec[key]\n            if key not in sum:\n                sum[key] = val\n            else:\n                sum[key] += val\n\n    return sum\n</code></pre>"},{"location":"reference/api/#tiedie.Kernel.diffuse","title":"<code>diffuse(vector, reverse=False)</code>","text":"<p>Diffuse input heats over the set of kernels, add to this object</p> Input <p>{'gene1': float(heat1)  'gene2' : float(heat2)   ... }</p> <p>Returns:</p> Type Description <p>Diffused heat vector</p> Source code in <code>tiedie/kernel.py</code> <pre><code>def diffuse(self, vector, reverse=False):\n    \"\"\"Diffuse input heats over the set of kernels, add to this object\n\n    Input:\n        {'gene1': float(heat1)\n         'gene2' : float(heat2)\n          ...\n        }\n\n    Returns:\n        Diffused heat vector\n    \"\"\"\n    # diffuse separately on each kernel (if more than one), and store.\n    return_vectors = []\n    for kernel in self.kernels:\n        # run diffusion on each constituent kernel\n        diffused_vector = self.kernel_multiply_one(kernel, vector)\n        return_vectors.append(diffused_vector)\n\n    return self.add_vectors(return_vectors)\n</code></pre>"},{"location":"reference/api/#tiedie.Kernel.get_labels","title":"<code>get_labels()</code>","text":"<p>Return the set of all node/gene labels used by this kernel object</p> Source code in <code>tiedie/kernel.py</code> <pre><code>def get_labels(self):\n    \"\"\"Return the set of all node/gene labels used by this kernel object\"\"\"\n    all_labels = set()\n    for label in self.labels:\n        all_labels = all_labels.union(set(self.labels[label]))\n\n    return all_labels\n</code></pre>"},{"location":"reference/api/#tiedie.Kernel.kernel_multiply_one","title":"<code>kernel_multiply_one(kernel, vector)</code>","text":"<p>Multiply the specified kernel by the supplied input heat vector.</p> Input <p>vector: A hash mapping gene labels to floating point values kernel: a single index for a specific kernel</p> <p>Returns:</p> Type Description <p>A hash of diffused heats, indexed by the same names as the</p> <p>input vector</p> Source code in <code>tiedie/kernel.py</code> <pre><code>def kernel_multiply_one(self, kernel, vector):\n    \"\"\"Multiply the specified kernel by the supplied input heat vector.\n\n    Input:\n        vector: A hash mapping gene labels to floating point values\n        kernel: a single index for a specific kernel\n\n    Returns:\n        A hash of diffused heats, indexed by the same names as the\n        input vector\n    \"\"\"\n\n    # Have to convert to ordered array format for the input vector\n    array = []\n    for label in self.labels[kernel]:\n        # Input heats may not actually be in the network.\n        # Check and initialize to zero if not\n        if label in vector:\n            array.append(vector[label])\n        else:\n            array.append(0)\n\n    # Matrix mulitply op\n    value = dot(self.kernels[kernel], array)\n\n    # Convert back to a hash and return diffused heats\n    return_vec = {}\n    idx = 0\n    for label in self.labels[kernel]:\n        return_vec[label] = float(value[idx])\n        idx += 1\n\n    return return_vec\n</code></pre>"},{"location":"reference/api/#pprdiffuser","title":"PprDiffuser","text":"<p>Personalized PageRank diffusion for network analysis.</p> Source code in <code>tiedie/ppr.py</code> <pre><code>class PprDiffuser:\n    \"\"\"Personalized PageRank diffusion for network analysis.\"\"\"\n\n    def __init__(self, network):\n        \"\"\"PprDiffuser: object to perform the Personalized PageRank Algorithm\n        This method creates the diffuser object from an networkx DiGraph() object,\n        which can then be used to diffuse vectors over this\n\n        Input:\n            - network : a network hash object\n        \"\"\"\n\n        # create a directed graph with NetworkX\n        self.G = nx.DiGraph()\n        # create a reversed graph for diffusion from the 'target' set\n        self.G_reversed = nx.DiGraph()\n        # convert network format to networkX Graph object\n        for source in network:\n            for i, t in network[source]:\n                self.G.add_edge(source, t)\n                self.G_reversed.add_edge(t, source)\n\n    def personal_page_rank(self, p_vector, reverse=False):\n        \"\"\"Personal_Page_Rank: Get the personal pagerank of the supplied input vector\n\n        Input:\n            - p_vector: A hash-map of input values for a selection (or all) nodes\n            (if supplied nodes aren't in the graph, they will be ignored)\n\n        Output:\n            - A vector of diffused heats in hash-map (key,value) format\n        \"\"\"\n        input_pvec = None\n        #  without initializing this vector the initial probabilities will be flat\n        # and this will be equivalent to standard page rank\n        if p_vector:\n            input_pvec = {}\n            # doesn't seem to be necessary for a non-zero epsilon now, but\n            # leave this as a place holder\n            epsilon = 0.0\n            for node in self.G.nodes(data=False):\n                if node in p_vector:\n                    input_pvec[node] = p_vector[node]\n                else:\n                    input_pvec[node] = epsilon\n\n        if reverse:\n            return nx.pagerank_numpy(self.G_reversed, 0.85, input_pvec)\n        else:\n            return nx.pagerank_numpy(self.G, 0.85, input_pvec)\n\n    def diffuse(self, p_vector, reverse=False):\n        \"\"\"Diffuse: perform generalized diffusion from the supplied input vector\n\n        Input:\n            - p_vector: A hash-map of input values for a selection (or all) nodes\n            (if supplied nodes aren't in the graph, they will be ignored)\n\n        Output:\n            - A vector of diffused heats in hash-map (key,value) format\n        \"\"\"\n        return self.personal_page_rank(p_vector, reverse)\n</code></pre>"},{"location":"reference/api/#tiedie.PprDiffuser-functions","title":"Functions","text":""},{"location":"reference/api/#tiedie.PprDiffuser.__init__","title":"<code>__init__(network)</code>","text":"<p>PprDiffuser: object to perform the Personalized PageRank Algorithm This method creates the diffuser object from an networkx DiGraph() object, which can then be used to diffuse vectors over this</p> Input <ul> <li>network : a network hash object</li> </ul> Source code in <code>tiedie/ppr.py</code> <pre><code>def __init__(self, network):\n    \"\"\"PprDiffuser: object to perform the Personalized PageRank Algorithm\n    This method creates the diffuser object from an networkx DiGraph() object,\n    which can then be used to diffuse vectors over this\n\n    Input:\n        - network : a network hash object\n    \"\"\"\n\n    # create a directed graph with NetworkX\n    self.G = nx.DiGraph()\n    # create a reversed graph for diffusion from the 'target' set\n    self.G_reversed = nx.DiGraph()\n    # convert network format to networkX Graph object\n    for source in network:\n        for i, t in network[source]:\n            self.G.add_edge(source, t)\n            self.G_reversed.add_edge(t, source)\n</code></pre>"},{"location":"reference/api/#tiedie.PprDiffuser.diffuse","title":"<code>diffuse(p_vector, reverse=False)</code>","text":"<p>Diffuse: perform generalized diffusion from the supplied input vector</p> Input <ul> <li>p_vector: A hash-map of input values for a selection (or all) nodes (if supplied nodes aren't in the graph, they will be ignored)</li> </ul> Output <ul> <li>A vector of diffused heats in hash-map (key,value) format</li> </ul> Source code in <code>tiedie/ppr.py</code> <pre><code>def diffuse(self, p_vector, reverse=False):\n    \"\"\"Diffuse: perform generalized diffusion from the supplied input vector\n\n    Input:\n        - p_vector: A hash-map of input values for a selection (or all) nodes\n        (if supplied nodes aren't in the graph, they will be ignored)\n\n    Output:\n        - A vector of diffused heats in hash-map (key,value) format\n    \"\"\"\n    return self.personal_page_rank(p_vector, reverse)\n</code></pre>"},{"location":"reference/api/#tiedie.PprDiffuser.personal_page_rank","title":"<code>personal_page_rank(p_vector, reverse=False)</code>","text":"<p>Personal_Page_Rank: Get the personal pagerank of the supplied input vector</p> Input <ul> <li>p_vector: A hash-map of input values for a selection (or all) nodes (if supplied nodes aren't in the graph, they will be ignored)</li> </ul> Output <ul> <li>A vector of diffused heats in hash-map (key,value) format</li> </ul> Source code in <code>tiedie/ppr.py</code> <pre><code>def personal_page_rank(self, p_vector, reverse=False):\n    \"\"\"Personal_Page_Rank: Get the personal pagerank of the supplied input vector\n\n    Input:\n        - p_vector: A hash-map of input values for a selection (or all) nodes\n        (if supplied nodes aren't in the graph, they will be ignored)\n\n    Output:\n        - A vector of diffused heats in hash-map (key,value) format\n    \"\"\"\n    input_pvec = None\n    #  without initializing this vector the initial probabilities will be flat\n    # and this will be equivalent to standard page rank\n    if p_vector:\n        input_pvec = {}\n        # doesn't seem to be necessary for a non-zero epsilon now, but\n        # leave this as a place holder\n        epsilon = 0.0\n        for node in self.G.nodes(data=False):\n            if node in p_vector:\n                input_pvec[node] = p_vector[node]\n            else:\n                input_pvec[node] = epsilon\n\n    if reverse:\n        return nx.pagerank_numpy(self.G_reversed, 0.85, input_pvec)\n    else:\n        return nx.pagerank_numpy(self.G, 0.85, input_pvec)\n</code></pre>"},{"location":"reference/api/#netbalancedpermuter","title":"NetBalancedPermuter","text":"<p>Encapsulates the permutation logic for an input heat set. Permutes Node scores with other nodes of similar network degree by sorting all nodes by degree, and binning them into blocks of a set size. Permutations are done only within blocks, so that the degree distribution of input nodes is preserved.</p> Source code in <code>tiedie/permute.py</code> <pre><code>class NetBalancedPermuter:\n    \"\"\"Encapsulates the permutation logic for an input heat set. Permutes\n    Node scores with other nodes of similar network degree by sorting\n    all nodes by degree, and binning them into blocks of a set size.\n    Permutations are done only within blocks, so that the degree distribution\n    of input nodes is preserved.\n    \"\"\"\n\n    def __init__(self, network, up_set):\n        \"\"\"Input:\n        network: net[source] = [(i, t)]\n        up_set: up_set[node] = score\n        \"\"\"\n\n        # store node-degrees for all in network\n        self.degrees = {}\n\n        # the set of initial nodes to permute within blocks: save it to the\n        # instantiated object here\n        self.nodes = up_set.keys()\n        # heuristic: block needs to be significantly larger than the input set size\n        BLOCK_MULTIPLE = 10\n        self.block_size = len(self.nodes) * BLOCK_MULTIPLE\n        self.scores = {}\n        for node in self.nodes:\n            # save the scores as a set of tuples\n            self.scores[(node, str(up_set[node]))] = 1\n\n        # Compute total degree for each node in the network\n        for source in network:\n            if source not in self.degrees:\n                self.degrees[source] = 0\n\n            for i, target in network[source]:\n                self.degrees[source] += 1\n\n                if target not in self.degrees:\n                    self.degrees[target] = 0\n                # add a degree for the incoming edge\n                self.degrees[target] += 1\n\n        # reverse-sort the degrees of all nodes in the network.\n        self.sorted_degrees = sorted(\n            self.degrees.items(), key=lambda x: x[1], reverse=True\n        )\n\n    def permute_block(self, block):\n        \"\"\"Take a block of nodes and randomly shuffle using python's random.shuffle method.\n\n        Input:\n\n            An array of node labels\n\n        Returns:\n            A hash mapping the original nodes to the nodes to swap with each.\n        \"\"\"\n        # make a copy\n        orig = copy(block)\n        b = copy(block)\n        map = {}\n        # shuffle the copy in-place with the random.shuffle() method.\n        shuffle(b)\n        for i in range(0, len(b)):\n            # build the mapping from original to new\n            map[orig[i]] = b[i]\n\n        return map\n\n    def permute_one(self):\n        \"\"\"Generate one permutation of scores for all nodes, and return a hash of { node : score }\n        for each.\n        \"\"\"\n        group_count = 0\n        permuted_scores = {}\n        # initialize a new block\n        block = []\n        for node, degree in self.sorted_degrees:\n            block.append(node)\n            group_count += 1\n            # reset every time we use the block size\n            if group_count % self.block_size == 0:\n                # permute the order of this &lt;block_size&gt; block\n                map = self.permute_block(block)\n                for node, score in self.scores:\n                    if node in map:\n                        permuted_scores[map[node]] = float(score)\n                block = []\n\n        return permuted_scores\n\n    def permute(self, iterations):\n        \"\"\"Generate an array of random permutations of node scores.\n\n        Input:\n            iteration: the number of permutations to generate\n\n        Returns:\n            an array of hashes: each hash indexes the nodes to permuted scores\n        \"\"\"\n        permuted = []\n        for i in range(0, iterations):\n            permuted.append(self.permute_one())\n\n        return permuted\n</code></pre>"},{"location":"reference/api/#tiedie.NetBalancedPermuter-functions","title":"Functions","text":""},{"location":"reference/api/#tiedie.NetBalancedPermuter.__init__","title":"<code>__init__(network, up_set)</code>","text":"<p>Input: network: net[source] = [(i, t)] up_set: up_set[node] = score</p> Source code in <code>tiedie/permute.py</code> <pre><code>def __init__(self, network, up_set):\n    \"\"\"Input:\n    network: net[source] = [(i, t)]\n    up_set: up_set[node] = score\n    \"\"\"\n\n    # store node-degrees for all in network\n    self.degrees = {}\n\n    # the set of initial nodes to permute within blocks: save it to the\n    # instantiated object here\n    self.nodes = up_set.keys()\n    # heuristic: block needs to be significantly larger than the input set size\n    BLOCK_MULTIPLE = 10\n    self.block_size = len(self.nodes) * BLOCK_MULTIPLE\n    self.scores = {}\n    for node in self.nodes:\n        # save the scores as a set of tuples\n        self.scores[(node, str(up_set[node]))] = 1\n\n    # Compute total degree for each node in the network\n    for source in network:\n        if source not in self.degrees:\n            self.degrees[source] = 0\n\n        for i, target in network[source]:\n            self.degrees[source] += 1\n\n            if target not in self.degrees:\n                self.degrees[target] = 0\n            # add a degree for the incoming edge\n            self.degrees[target] += 1\n\n    # reverse-sort the degrees of all nodes in the network.\n    self.sorted_degrees = sorted(\n        self.degrees.items(), key=lambda x: x[1], reverse=True\n    )\n</code></pre>"},{"location":"reference/api/#tiedie.NetBalancedPermuter.permute","title":"<code>permute(iterations)</code>","text":"<p>Generate an array of random permutations of node scores.</p> Input <p>iteration: the number of permutations to generate</p> <p>Returns:</p> Type Description <p>an array of hashes: each hash indexes the nodes to permuted scores</p> Source code in <code>tiedie/permute.py</code> <pre><code>def permute(self, iterations):\n    \"\"\"Generate an array of random permutations of node scores.\n\n    Input:\n        iteration: the number of permutations to generate\n\n    Returns:\n        an array of hashes: each hash indexes the nodes to permuted scores\n    \"\"\"\n    permuted = []\n    for i in range(0, iterations):\n        permuted.append(self.permute_one())\n\n    return permuted\n</code></pre>"},{"location":"reference/api/#tiedie.NetBalancedPermuter.permute_block","title":"<code>permute_block(block)</code>","text":"<p>Take a block of nodes and randomly shuffle using python's random.shuffle method.</p> <p>Input:</p> <pre><code>An array of node labels\n</code></pre> <p>Returns:</p> Type Description <p>A hash mapping the original nodes to the nodes to swap with each.</p> Source code in <code>tiedie/permute.py</code> <pre><code>def permute_block(self, block):\n    \"\"\"Take a block of nodes and randomly shuffle using python's random.shuffle method.\n\n    Input:\n\n        An array of node labels\n\n    Returns:\n        A hash mapping the original nodes to the nodes to swap with each.\n    \"\"\"\n    # make a copy\n    orig = copy(block)\n    b = copy(block)\n    map = {}\n    # shuffle the copy in-place with the random.shuffle() method.\n    shuffle(b)\n    for i in range(0, len(b)):\n        # build the mapping from original to new\n        map[orig[i]] = b[i]\n\n    return map\n</code></pre>"},{"location":"reference/api/#tiedie.NetBalancedPermuter.permute_one","title":"<code>permute_one()</code>","text":"<p>Generate one permutation of scores for all nodes, and return a hash of { node : score } for each.</p> Source code in <code>tiedie/permute.py</code> <pre><code>def permute_one(self):\n    \"\"\"Generate one permutation of scores for all nodes, and return a hash of { node : score }\n    for each.\n    \"\"\"\n    group_count = 0\n    permuted_scores = {}\n    # initialize a new block\n    block = []\n    for node, degree in self.sorted_degrees:\n        block.append(node)\n        group_count += 1\n        # reset every time we use the block size\n        if group_count % self.block_size == 0:\n            # permute the order of this &lt;block_size&gt; block\n            map = self.permute_block(block)\n            for node, score in self.scores:\n                if node in map:\n                    permuted_scores[map[node]] = float(score)\n            block = []\n\n    return permuted_scores\n</code></pre>"},{"location":"reference/api/#utility-functions","title":"Utility Functions","text":"Parse input heats file in form <p> <p>Returns:</p> Type Description <ul> <li>Two hashes: one indexing by gene and storing the input heats, and one storing the input signs</li> </ul> Source code in <code>tiedie/util.py</code> <pre><code>def parse_heats(file, network_nodes=None):\n    \"\"\"Parse input heats file in form:\n        &lt;gene&gt; &lt;heat&gt; &lt;perturbation/activity sign (+/-)&gt;\n\n    Returns:\n        - Two hashes: one indexing by gene and storing the input heats, and one storing the input signs\n    \"\"\"\n\n    heats = {}\n    signs = {}\n    fh = None\n    try:\n        fh = open(file)\n    except OSError as e:\n        raise Exception(\"Error: can't open file: \" + file) from e\n\n    lineno = 1\n    for line in fh:\n        parts = line.rstrip().split('\\t')\n        if len(parts) &gt; 2:\n            prot, heat, sign = line.rstrip().split('\\t')\n\n            # provide a warning if node not in the network\n            if network_nodes and prot not in network_nodes:\n                sys.stderr.write(\n                    'Warning: input heat node '\n                    + prot\n                    + ' not in the network and will be ignored...\\n'\n                )\n                continue\n\n            # input validation for heat values\n            try:\n                heats[prot] = float(heat)\n            except ValueError as e:\n                raise Exception(\n                    'Error: non float heat value on line '\n                    + str(lineno)\n                    + ' gene '\n                    + prot\n                ) from e\n\n            # input validation for input signs\n            if sign != '+' and sign != '-':\n                raise Exception(\n                    'Error: invalid value for heat sign on line '\n                    + str(lineno)\n                    + sign\n                )\n\n            signs[prot] = sign\n        else:\n            heats[parts[0]] = float(parts[1])\n\n        lineno += 1\n\n    fh.close()\n    return (heats, signs)\n</code></pre> <p>Build a directed network from a .sif file.</p> Inputs <p>A network in .sif format, tab-separated ( ) <p>Returns:</p> Name Type Description <p>A network in hash key format, i.e. convert two lines of a file:  <code>To</code> <p>{'source': set( (interaction, target1), (interaction, target2) )</p> Source code in <code>tiedie/util.py</code> <pre><code>def parse_net(network):\n    \"\"\"Build a directed network from a .sif file.\n\n    Inputs:\n        A network in .sif format, tab-separated (&lt;source&gt; &lt;interaction&gt; &lt;target&gt;)\n\n    Returns:\n        A network in hash key format, i.e. convert two lines of a file:\n            &lt;source&gt;    &lt;interaction1&gt;  &lt;target1&gt;\n            &lt;source&gt;    &lt;interaction2&gt;  &lt;target2&gt;\n        To:\n            {'source': set( (interaction, target1), (interaction, target2) )\n    \"\"\"\n    net = {}\n    for line in open(network):\n        parts = line.rstrip().split('\\t')\n        source = parts[0]\n        interaction = parts[1]\n        target = parts[2]\n\n        if source not in net:\n            net[source] = set()\n\n        net[source].add((interaction, target))\n\n    return net\n</code></pre> <p>Normalize absolute value sum of data hash to 1000</p> Source code in <code>tiedie/util.py</code> <pre><code>def normalize_heats(data):\n    \"\"\"Normalize absolute value sum of data hash to 1000\"\"\"\n    FACTOR = 1000\n    normalized = {}\n    signs = {}\n    sum = 0.0\n    for event, val in data.items():\n        sum += abs(val)\n\n    for event, val in data.items():\n        sign = '+'\n        if val &lt; 0:\n            sign = '-'\n        normalized[event] = FACTOR * abs(val) / sum\n        signs[event] = sign\n\n    return normalized\n</code></pre> <p>Take a network in hash-key format and return a set containing the nodes in it.</p> Source code in <code>tiedie/util.py</code> <pre><code>def get_network_nodes(network):\n    \"\"\"Take a network in hash-key format and return a set containing the\n    nodes in it.\n    \"\"\"\n    nodes = set()\n    for s in network:\n        nodes.add(s)\n        for i, t in network[s]:\n            nodes.add(t)\n    return nodes\n</code></pre> <p>Use the min(diffused1, diffused2) function to return a list of genes that fall above that cutoff. Input:     diffused heats for each set, and the numeric cutoff value</p> <p>Returns:</p> Type Description <p>a list of genes above the cutoff, a hash of minimum heat values</p> Source code in <code>tiedie/util.py</code> <pre><code>def filter_linkers(up_heats_diffused, down_heats_diffused, cutoff):\n    \"\"\"Use the min(diffused1, diffused2) function to return a list of genes\n    that fall above that cutoff.\n    Input:\n        diffused heats for each set, and the numeric cutoff value\n\n    Returns:\n        a list of genes above the cutoff, a hash of minimum heat values\n    \"\"\"\n    linkers = {}\n    filtered = []\n    if down_heats_diffused is None:\n        # trivially: if this is a single list of diffused values, just return it\n        return (up_heats_diffused.keys(), up_heats_diffused)\n\n    for node in up_heats_diffused:\n        if node not in down_heats_diffused:\n            # it doesn't make the cut if it's not in both sets\n            continue\n        min_heat = min(up_heats_diffused[node], down_heats_diffused[node])\n        linkers[node] = min_heat\n        if min_heat &gt; cutoff:\n            filtered.append(node)\n\n    return (filtered, linkers)\n</code></pre> <p>Find a heat threshold that yields a linker set of the given size.</p> <p>Parameters:</p> Name Type Description Default <code>source_set</code> <p>Set of source node names.</p> required <code>target_set</code> <p>Set of target node names.</p> required <code>up_heat_diffused</code> <p>Dict mapping nodes to upstream diffused heat values.</p> required <code>down_heat_diffused</code> <p>Dict mapping nodes to downstream diffused heat values.</p> required <code>size</code> <p>Relative size of the linker set (compared to input set size).</p> required <p>Returns:</p> Type Description <p>Tuple of (cutoff_threshold, relevance_score).</p> Example <pre><code>find_linker_cutoff(\n    source_set={\"A\", \"B\"},\n    target_set={\"X\", \"Y\"},\n    up_heat_diffused={\"A\": 1.0, \"B\": 1.1, \"C\": 0.5, \"D\": 0.4},\n    down_heat_diffused={\"X\": 2.0, \"Y\": 2.1, \"C\": 0.7, \"D\": 0.5},\n    size=0.2\n)\n# Returns: (0.4999, 0.1667)\n</code></pre> Source code in <code>tiedie/util.py</code> <pre><code>def find_linker_cutoff(\n    source_set, target_set, up_heat_diffused, down_heat_diffused, size\n):\n    \"\"\"Find a heat threshold that yields a linker set of the given size.\n\n    Args:\n        source_set: Set of source node names.\n        target_set: Set of target node names.\n        up_heat_diffused: Dict mapping nodes to upstream diffused heat values.\n        down_heat_diffused: Dict mapping nodes to downstream diffused heat values.\n        size: Relative size of the linker set (compared to input set size).\n\n    Returns:\n        Tuple of (cutoff_threshold, relevance_score).\n\n    Example:\n        ```python\n        find_linker_cutoff(\n            source_set={\"A\", \"B\"},\n            target_set={\"X\", \"Y\"},\n            up_heat_diffused={\"A\": 1.0, \"B\": 1.1, \"C\": 0.5, \"D\": 0.4},\n            down_heat_diffused={\"X\": 2.0, \"Y\": 2.1, \"C\": 0.7, \"D\": 0.5},\n            size=0.2\n        )\n        # Returns: (0.4999, 0.1667)\n        ```\n    \"\"\"\n    if down_heat_diffused is None:\n        # diffusing from a single source (i.e. not TieDIE but the HotNet algorithm, for comparison)\n        cutoff, score = find_linker_cutoff_single(\n            source_set, up_heat_diffused, size\n        )\n    else:\n        try:\n            cutoff, score = find_linker_cutoff_multi(\n                source_set,\n                target_set,\n                up_heat_diffused,\n                down_heat_diffused,\n                size,\n            )\n        except Exception as e:  # noqa: BLE001 - intentional fallback with warning\n            global _linker_cutoff_error_count, _linker_cutoff_error_shown\n            _linker_cutoff_error_count += 1\n            if not _linker_cutoff_error_shown:\n                warnings.warn(\n                    f'find_linker_cutoff failed: {e}',\n                    stacklevel=2,\n                )\n                _linker_cutoff_error_shown = True\n            return (0, 0)\n\n    return (cutoff, score)\n</code></pre> <p>Extract edges where both endpoints are in the given node subset.</p> <p>Parameters:</p> Name Type Description Default <code>network</code> <p>Dict mapping source nodes to sets of (interaction, target) tuples.</p> required <code>subnet_nodes</code> <p>Set of nodes to use for edge selection.</p> required <p>Returns:</p> Type Description <p>Set of (source, target) edge tuples where both nodes are in subnet_nodes.</p> Example <pre><code>network = {\n    'S1': {('a&gt;', 'T1')},\n    'S2': {('a&gt;', 'T2')},\n    'T1': {('t|', 'T2')},\n    'T2': {('a&gt;', 'T1')},\n    'T3': {('t&gt;', 'G5')},\n}\nconnected_subnets(network, {'S1', 'T1', 'T2', 'T3', 'G5'})\n# Returns: {('S1', 'T1'), ('T1', 'T2'), ('T2', 'T1')}\n</code></pre> Source code in <code>tiedie/util.py</code> <pre><code>def connected_subnets(network, subnet_nodes):\n    \"\"\"Extract edges where both endpoints are in the given node subset.\n\n    Args:\n        network: Dict mapping source nodes to sets of (interaction, target) tuples.\n        subnet_nodes: Set of nodes to use for edge selection.\n\n    Returns:\n        Set of (source, target) edge tuples where both nodes are in subnet_nodes.\n\n    Example:\n        ```python\n        network = {\n            'S1': {('a&gt;', 'T1')},\n            'S2': {('a&gt;', 'T2')},\n            'T1': {('t|', 'T2')},\n            'T2': {('a&gt;', 'T1')},\n            'T3': {('t&gt;', 'G5')},\n        }\n        connected_subnets(network, {'S1', 'T1', 'T2', 'T3', 'G5'})\n        # Returns: {('S1', 'T1'), ('T1', 'T2'), ('T2', 'T1')}\n        ```\n    \"\"\"\n    edgelist = set()\n    ugraph = set()\n\n    for s in network:\n        for i, t in network[s]:\n            # ignore self-links\n            if s == t:\n                continue\n            if s in subnet_nodes and t in subnet_nodes:\n                edgelist.add((s, t))\n                if (t, s) not in edgelist:\n                    ugraph.add((s, t))\n\n    # use networkx to find the largest connected sub graph\n    G = nx.Graph()\n    G.add_edges_from(list(ugraph))\n    # get the biggest connected component, add edges between all\n    validated_edges = set()\n    for component in nx.connected_components(G):\n        validated_nodes = component\n        for s, t in edgelist:\n            # validate both nodes\n            if s in validated_nodes and t in validated_nodes:\n                validated_edges.add((s, t))\n\n        break\n\n    return validated_edges\n</code></pre> <p>Map undirected edges to the network to form a subnetwork in the hash-key directed network format</p> Input <p>edge_list: edges in (s,t) format network: network in {source:set( (int, target), ... )</p> <p>Returns:</p> Type Description <p>Subnetwork in the data structure format of network input</p> Source code in <code>tiedie/util.py</code> <pre><code>def map_ugraph_to_network(edge_list, network):\n    \"\"\"Map undirected edges to the network to form a subnetwork\n    in the hash-key directed network format\n\n    Input:\n        edge_list: edges in (s,t) format\n        network: network in {source:set( (int, target), ... )\n\n    Returns:\n        Subnetwork in the data structure format of network input\n    \"\"\"\n\n    subnetwork = {}\n\n    for s, t in edge_list:\n        # find this equivalent edge(s) in the directed network\n        # edges:\n        if s in network:\n            for i, nt in network[s]:\n                if nt == t:\n                    if s not in subnetwork:\n                        subnetwork[s] = set()\n                    subnetwork[s].add((i, t))\n\n        if t in network:\n            for i, nt in network[t]:\n                if nt == s:\n                    if t not in subnetwork:\n                        subnetwork[t] = set()\n                    subnetwork[t].add((i, s))\n\n    return subnetwork\n</code></pre>"}]}